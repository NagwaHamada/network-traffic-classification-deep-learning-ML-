{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07e821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92385\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94       469\n",
      "           1       0.95      0.45      0.61       117\n",
      "           2       0.65      0.48      0.55       868\n",
      "           3       0.91      0.97      0.94       721\n",
      "           4       0.78      0.94      0.85      2266\n",
      "           5       0.93      0.95      0.94       668\n",
      "           6       0.97      1.00      0.98      5434\n",
      "           7       1.00      0.80      0.89       158\n",
      "           8       0.00      0.00      0.00         4\n",
      "           9       0.00      0.00      0.00         6\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.00      0.00      0.00         4\n",
      "          12       0.00      0.00      0.00         5\n",
      "          13       0.00      0.00      0.00         6\n",
      "          14       0.00      0.00      0.00         5\n",
      "          15       0.00      0.00      0.00         7\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       1.00      0.14      0.25         7\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.00      0.00      0.00         4\n",
      "          20       0.00      0.00      0.00         5\n",
      "          21       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       1.00      0.38      0.55         8\n",
      "          24       0.00      0.00      0.00         7\n",
      "          25       0.00      0.00      0.00         8\n",
      "          26       0.83      0.67      0.74        15\n",
      "          27       0.97      0.89      0.93        36\n",
      "          28       0.93      0.81      0.86        31\n",
      "          29       0.89      0.64      0.74        50\n",
      "          30       0.91      0.89      0.90       106\n",
      "          31       0.62      0.48      0.54        27\n",
      "          32       0.88      0.52      0.66        88\n",
      "          33       0.73      0.47      0.57        77\n",
      "          34       0.96      0.80      0.87       168\n",
      "          35       1.00      0.70      0.82       145\n",
      "          36       0.95      0.81      0.88       113\n",
      "          37       0.96      0.84      0.90       200\n",
      "          38       0.97      0.96      0.97       542\n",
      "          39       0.99      0.99      0.99       992\n",
      "          40       0.98      0.99      0.99       883\n",
      "          41       0.97      0.99      0.98       570\n",
      "          42       0.98      0.97      0.98       646\n",
      "          43       0.91      0.93      0.92       127\n",
      "          44       0.95      0.97      0.96       373\n",
      "          45       0.96      0.91      0.94       104\n",
      "          46       0.95      0.96      0.95       168\n",
      "          47       0.98      0.96      0.97       181\n",
      "          48       0.97      0.97      0.97       259\n",
      "          49       0.96      0.93      0.95       102\n",
      "          50       0.97      0.98      0.98       100\n",
      "          51       0.99      0.98      0.98        89\n",
      "          52       0.75      0.33      0.46         9\n",
      "          53       0.96      0.96      0.96       180\n",
      "          54       0.97      0.98      0.97       177\n",
      "          55       0.95      0.93      0.94        40\n",
      "          56       0.97      0.98      0.97       135\n",
      "          57       0.99      0.96      0.97        78\n",
      "          58       0.99      0.93      0.96        90\n",
      "          59       0.98      0.96      0.97       135\n",
      "          60       0.87      0.72      0.79        18\n",
      "          61       0.98      0.91      0.94        44\n",
      "          62       0.98      0.91      0.94        46\n",
      "          63       1.00      0.93      0.96        41\n",
      "          64       0.95      0.92      0.93        38\n",
      "          65       0.97      0.95      0.96       129\n",
      "          66       1.00      0.94      0.97        31\n",
      "          67       0.99      0.92      0.95       122\n",
      "          68       1.00      0.94      0.97        31\n",
      "          69       0.99      0.98      0.98        98\n",
      "          70       0.99      0.96      0.98       111\n",
      "          71       0.94      0.87      0.91        39\n",
      "          72       1.00      0.89      0.94        54\n",
      "          73       0.97      0.99      0.98       154\n",
      "          74       0.99      0.97      0.98        88\n",
      "          75       1.00      0.97      0.98        33\n",
      "          76       0.99      0.99      0.99       110\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       1.00      1.00      1.00        38\n",
      "          79       0.00      0.00      0.00         2\n",
      "          81       0.94      0.98      0.96        50\n",
      "          83       0.99      0.96      0.97        73\n",
      "          85       0.00      0.00      0.00         3\n",
      "          86       0.00      0.00      0.00         4\n",
      "          87       1.00      1.00      1.00         2\n",
      "          89       1.00      0.98      0.99        42\n",
      "          90       0.98      1.00      0.99        42\n",
      "          92       1.00      1.00      1.00        47\n",
      "          93       0.00      0.00      0.00         1\n",
      "          94       0.00      0.00      0.00         1\n",
      "          98       1.00      1.00      1.00         1\n",
      "         100       0.50      0.50      0.50         2\n",
      "         101       0.00      0.00      0.00         1\n",
      "         102       1.00      0.25      0.40         4\n",
      "         103       0.00      0.00      0.00         1\n",
      "         104       0.00      0.00      0.00         2\n",
      "         105       0.50      0.33      0.40         3\n",
      "         108       0.00      0.00      0.00         1\n",
      "         110       0.97      1.00      0.98        30\n",
      "         111       0.00      0.00      0.00         0\n",
      "         112       0.00      0.00      0.00         0\n",
      "         113       0.00      0.00      0.00         0\n",
      "         115       0.00      0.00      0.00         1\n",
      "         117       0.00      0.00      0.00         1\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.00      0.00      0.00         1\n",
      "         134       0.00      0.00      0.00         1\n",
      "         135       0.00      0.00      0.00         2\n",
      "         142       0.00      0.00      0.00         1\n",
      "         143       0.00      0.00      0.00         0\n",
      "         144       0.00      0.00      0.00         1\n",
      "         146       0.50      1.00      0.67         1\n",
      "         147       1.00      0.25      0.40         4\n",
      "         148       0.00      0.00      0.00         2\n",
      "         149       0.00      0.00      0.00         1\n",
      "         150       0.00      0.00      0.00         1\n",
      "         159       1.00      1.00      1.00         1\n",
      "         162       0.00      0.00      0.00         1\n",
      "         163       0.00      0.00      0.00         1\n",
      "         164       0.00      0.00      0.00         2\n",
      "         166       1.00      1.00      1.00         1\n",
      "         168       0.00      0.00      0.00         1\n",
      "         170       1.00      0.33      0.50         3\n",
      "         173       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         1\n",
      "         179       0.25      1.00      0.40         1\n",
      "         180       1.00      1.00      1.00         1\n",
      "         181       0.50      0.50      0.50         2\n",
      "         183       0.00      0.00      0.00         1\n",
      "         185       0.00      0.00      0.00         0\n",
      "         186       0.00      0.00      0.00         2\n",
      "         194       0.00      0.00      0.00         1\n",
      "         195       0.00      0.00      0.00         1\n",
      "         199       0.00      0.00      0.00         1\n",
      "         202       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         2\n",
      "         204       0.96      0.98      0.97        51\n",
      "         205       0.00      0.00      0.00         1\n",
      "         207       0.00      0.00      0.00         1\n",
      "         208       1.00      1.00      1.00         1\n",
      "         210       1.00      1.00      1.00         1\n",
      "         211       0.00      0.00      0.00         0\n",
      "         212       0.00      0.00      0.00         2\n",
      "         213       0.00      0.00      0.00         3\n",
      "         214       0.50      0.50      0.50         2\n",
      "         216       0.00      0.00      0.00         0\n",
      "         221       0.67      0.67      0.67         3\n",
      "         223       0.00      0.00      0.00         1\n",
      "         225       0.50      1.00      0.67         1\n",
      "         227       0.00      0.00      0.00         0\n",
      "         229       0.00      0.00      0.00         2\n",
      "         230       1.00      1.00      1.00         2\n",
      "         236       0.00      0.00      0.00         1\n",
      "         244       0.00      0.00      0.00         0\n",
      "         247       0.00      0.00      0.00         1\n",
      "         248       0.00      0.00      0.00         0\n",
      "         249       0.00      0.00      0.00         1\n",
      "         250       0.00      0.00      0.00         1\n",
      "         252       1.00      1.00      1.00         1\n",
      "         253       0.00      0.00      0.00         1\n",
      "         254       0.00      0.00      0.00         1\n",
      "         255       1.00      0.50      0.67         2\n",
      "         260       0.33      1.00      0.50         1\n",
      "         271       0.00      0.00      0.00         0\n",
      "         276       0.00      0.00      0.00         4\n",
      "         278       0.00      0.00      0.00         1\n",
      "         281       0.25      0.50      0.33         2\n",
      "         286       0.00      0.00      0.00         1\n",
      "         287       0.00      0.00      0.00         3\n",
      "         288       0.00      0.00      0.00         1\n",
      "         291       0.00      0.00      0.00         2\n",
      "         293       0.50      1.00      0.67         1\n",
      "         303       1.00      1.00      1.00         1\n",
      "         304       1.00      1.00      1.00         2\n",
      "         305       1.00      0.33      0.50         3\n",
      "         306       0.00      0.00      0.00         1\n",
      "         307       0.00      0.00      0.00         2\n",
      "         309       0.50      1.00      0.67         1\n",
      "         313       0.50      1.00      0.67         1\n",
      "         315       0.95      0.97      0.96        40\n",
      "         316       0.00      0.00      0.00         2\n",
      "         318       0.00      0.00      0.00         2\n",
      "         319       0.33      0.50      0.40         2\n",
      "         320       0.00      0.00      0.00         1\n",
      "         321       0.50      1.00      0.67         1\n",
      "         325       0.96      1.00      0.98        50\n",
      "         327       0.00      0.00      0.00         2\n",
      "         328       0.00      0.00      0.00         0\n",
      "         334       1.00      0.50      0.67         2\n",
      "         337       1.00      0.33      0.50         3\n",
      "         341       1.00      0.33      0.50         3\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       0.33      0.50      0.40         2\n",
      "         344       0.00      0.00      0.00         1\n",
      "         345       0.00      0.00      0.00         2\n",
      "         349       0.00      0.00      0.00         0\n",
      "         358       0.00      0.00      0.00         1\n",
      "         359       0.90      0.92      0.91        38\n",
      "         360       1.00      1.00      1.00         1\n",
      "         361       0.00      0.00      0.00         2\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.00      0.00      0.00         1\n",
      "         367       0.00      0.00      0.00         2\n",
      "         369       0.92      0.96      0.94        48\n",
      "         372       1.00      0.67      0.80         3\n",
      "         374       0.90      1.00      0.95        38\n",
      "         379       0.00      0.00      0.00         1\n",
      "         381       0.00      0.00      0.00         0\n",
      "         382       0.00      0.00      0.00         2\n",
      "         389       0.00      0.00      0.00         3\n",
      "         392       1.00      0.50      0.67         2\n",
      "         393       1.00      1.00      1.00        38\n",
      "         395       0.88      1.00      0.94        29\n",
      "         398       0.92      1.00      0.96        44\n",
      "         401       0.00      0.00      0.00         0\n",
      "         402       0.00      0.00      0.00         0\n",
      "         403       0.00      0.00      0.00         1\n",
      "         429       0.00      0.00      0.00         1\n",
      "         432       1.00      0.50      0.67         2\n",
      "         435       0.00      0.00      0.00         1\n",
      "         437       0.89      1.00      0.94        32\n",
      "         440       0.00      0.00      0.00         1\n",
      "         442       0.00      0.00      0.00         1\n",
      "         447       0.97      1.00      0.99        36\n",
      "         457       0.00      0.00      0.00         2\n",
      "         461       0.00      0.00      0.00         4\n",
      "         467       0.94      1.00      0.97        32\n",
      "\n",
      "    accuracy                           0.92     20000\n",
      "   macro avg       0.47      0.44      0.44     20000\n",
      "weighted avg       0.92      0.92      0.92     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ascom\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ascom\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ascom\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ascom\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ascom\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ascom\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def load_and_preprocess_data(file_paths, chunk_size=100000):\n",
    "    dataframes = []\n",
    "\n",
    "    for file in file_paths:\n",
    "        # Read CSV file in chunks\n",
    "        chunk_iter = pd.read_csv(file, chunksize=chunk_size)\n",
    "\n",
    "        for chunk in chunk_iter:\n",
    "            # Handle missing values for numeric columns\n",
    "            numeric_cols = chunk.select_dtypes(include=[np.number]).columns\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            chunk[numeric_cols] = imputer.fit_transform(chunk[numeric_cols])\n",
    "\n",
    "            # Handle categorical columns \n",
    "            non_numeric_cols = chunk.select_dtypes(exclude=[np.number]).columns\n",
    "            for col in non_numeric_cols:\n",
    "                encoder = LabelEncoder()\n",
    "                chunk[col] = encoder.fit_transform(chunk[col].astype(str))\n",
    "\n",
    "            \n",
    "            dataframes.append(chunk)\n",
    "\n",
    "    \n",
    "    data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    target_column = 'Label' if 'Label' in data.columns else 'Class'\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Downsample the data if it's too large\n",
    "    max_samples = 100000\n",
    "    if len(X) > max_samples:\n",
    "        sampled_indices = np.random.choice(len(X), max_samples, replace=False)\n",
    "        X = X.iloc[sampled_indices]\n",
    "        y = y.iloc[sampled_indices]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "file_paths = [\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\1\\\\capture20110810.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\2\\\\capture20110811.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\3\\\\capture20110812.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\4\\\\capture20110815.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\5\\\\capture20110815-2.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\6\\\\capture20110816.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\7\\\\capture20110816-2.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\8\\\\capture20110816-3.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\9\\\\capture20110817.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\10\\\\capture20110818.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\11\\\\capture20110818-2.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\12\\\\capture20110819.binetflow',\n",
    "    'C:\\\\Users\\\\ascom\\\\CTU-13\\\\13\\\\capture20110815-3.binetflow'\n",
    "]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_and_preprocess_data(file_paths)\n",
    "\n",
    "train_and_evaluate_models(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65c88cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ascom\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m436991/436991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 419us/step\n",
      "\u001b[1m187282/187282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 810us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99   5860113\n",
      "           1       0.95      0.35      0.52    132897\n",
      "\n",
      "    accuracy                           0.99   5993010\n",
      "   macro avg       0.97      0.68      0.75   5993010\n",
      "weighted avg       0.98      0.99      0.98   5993010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_paths):\n",
    "    dataframes = []\n",
    "    for file_path in file_paths:\n",
    "        for chunk in pd.read_csv(file_path, chunksize=10000):\n",
    "            columns_to_keep = ['Dur', 'TotPkts', 'TotBytes', 'SrcBytes', 'Label']\n",
    "            chunk = chunk[columns_to_keep]\n",
    "            chunk.dropna(inplace=True)  \n",
    "            \n",
    "            # Convert categorical labels to binary\n",
    "            chunk['Label'] = chunk['Label'].apply(lambda x: 1 if 'Botnet' in x else 0)\n",
    "            dataframes.append(chunk)\n",
    "\n",
    "\n",
    "    data = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "   \n",
    "    X = data.drop('Label', axis=1)\n",
    "    y = data['Label']\n",
    "    \n",
    "    # Normalize features\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    \n",
    "    return train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "def build_autoencoder(input_dim):\n",
    "    encoder = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu')\n",
    "    ])\n",
    "    decoder = Sequential([\n",
    "        Dense(32, activation='relu', input_dim=16),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(input_dim, activation='sigmoid')\n",
    "    ])\n",
    "    autoencoder = Sequential([encoder, decoder])\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "def extract_features_with_dnn(X_train, X_test):\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = build_autoencoder(input_dim)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train, X_train, epochs=10, batch_size=32, verbose=0)  \n",
    "    encoder = model.layers[0]  \n",
    "    return encoder.predict(X_train), encoder.predict(X_test)\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train, y_train)\n",
    "    y_pred = gbc.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred)) \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_dir = r\"C:\\Users\\ascom\\CTU-13\"\n",
    "    file_paths = [\n",
    "        os.path.join(dataset_dir, subdir, file)\n",
    "        for subdir in os.listdir(dataset_dir)\n",
    "        for file in os.listdir(os.path.join(dataset_dir, subdir))\n",
    "        if file.endswith('.binetflow')\n",
    "    ]\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data(file_paths)\n",
    "    X_train_features, X_test_features = extract_features_with_dnn(X_train, X_test)\n",
    "    train_and_evaluate_model(X_train_features, X_test_features, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e77a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
